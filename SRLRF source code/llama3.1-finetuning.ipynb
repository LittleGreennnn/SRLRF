{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db870343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 3100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model/LLM-Research/Meta-Llama-3___1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11560ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, # Suggested 8,16,32,64,128\n",
    "    target_modules= [\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                                    \"gate_proj\",\"up_proj\",\"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout =0,#Supports any,but =0 is optimized\n",
    "    bias = \"none\", #Supports any,but =“none” is optimized\n",
    "    #[NEW] “unsloth” uses 30% less VRAM,fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", #True or \"unsloth” for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,# We support rank stabilized LoRA \n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "# from datasets import load_dataset\n",
    "# train_dataset = load_dataset('json', data_files='data/split9-1/train_dataset.json', split='train')\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# 加载三个数据集\n",
    "dataset1 = load_dataset('csv', data_files='data/dataset_5fold_1/dataset_fold_1.csv')['train']\n",
    "dataset2 = load_dataset('csv', data_files='data/dataset_5fold_1/dataset_fold_2.csv')['train']\n",
    "dataset3 = load_dataset('csv', data_files='data/dataset_5fold_1/dataset_fold_3.csv')['train']\n",
    "# dataset4 = load_dataset('csv', data_files='data/dataset_5fold_1/dataset_fold_4.csv')['train']\n",
    "\n",
    "# 加载验证集\n",
    "dataset5 = load_dataset('csv', data_files='data/dataset_5fold_1/dataset_fold_4.csv')['train']\n",
    "\n",
    "# 合并数据集\n",
    "train_dataset = concatenate_datasets([dataset1, dataset2, dataset3])\n",
    "\n",
    "# 打印合并后的数据集\n",
    "print(train_dataset)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"report\",\"code\",\"label\"])\n",
    "validation_dataset = dataset5.remove_columns([\"report\",\"code\",\"label\"])\n",
    "\n",
    "# 打印删除无关列的数据集\n",
    "print(train_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length= max_seq_length,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    " \n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 自动截断code/input\n",
    "\n",
    "# MAX_LENGTH = 4000  # Llama3.1支持128k上下文\n",
    "\n",
    "# def tokenize_and_truncate(data_point):\n",
    "#     # 构建初始的 full_prompt\n",
    "#     full_prompt = f\"\"\"You are a developer of the GCC compiler. Your job is to categorize bug reports. You are given a snippet of code that triggers the bug and a description of the bug.\n",
    "# The bug reports are categorized as follows:'code-simplification-optimization-defects','control-flow-optimization-defects','data-flow-analysis-optimization-defects','infrastructure-defects','interprocedural-optimization-defects','memory-optimization-defects','numerical-analysis-optimization-defects','vectorization-defects'.\n",
    "# ### Code Snippet:\n",
    "# {data_point[\"code\"]}\n",
    "# ### Bug Description:\n",
    "# {data_point[\"report\"]}\n",
    "# ### Response:\n",
    "# {data_point[\"category\"]}\n",
    "# \"\"\"\n",
    "    \n",
    "#     # Tokenize整个prompt\n",
    "#     tokenized_prompt = tokenizer(full_prompt)\n",
    "#     token_length = len(tokenized_prompt['input_ids'])\n",
    "\n",
    "#     # 如果超过128k tokens，截断处理\n",
    "#     if token_length > MAX_LENGTH:\n",
    "#         # Tokenize code 和 input 部分\n",
    "#         tokenized_code = tokenizer(data_point[\"code\"], truncation=False)\n",
    "#         tokenized_input = tokenizer(data_point[\"report\"], truncation=False)\n",
    "\n",
    "#         # 分别计算 code 和 input 的 token 长度\n",
    "#         code_token_length = len(tokenized_code['input_ids'])\n",
    "#         input_token_length = len(tokenized_input['input_ids'])\n",
    "\n",
    "#         # 保留的长度 = MAX_LENGTH - (固定部分的token长度，即非code和input部分)\n",
    "#         fixed_prompt = f\"\"\"You are a developer of the GCC compiler. Your job is to categorize bug reports. You are given a snippet of code that triggers the bug and a description of the bug.\n",
    "# The bug reports are categorized as follows:'code-simplification-optimization-defects','control-flow-optimization-defects','data-flow-analysis-optimization-defects','infrastructure-defects','interprocedural-optimization-defects','memory-optimization-defects','numerical-analysis-optimization-defects','vectorization-defects'.\n",
    "# ### Code Snippet:\n",
    "# ### Bug Description:\n",
    "# ### Response:\n",
    "# {data_point[\"category\"]}\n",
    "# \"\"\"\n",
    "#         fixed_token_length = len(tokenizer(fixed_prompt)['input_ids'])\n",
    "#         remaining_length = MAX_LENGTH - fixed_token_length\n",
    "\n",
    "#         # 优先截断 code 和 input\n",
    "#         if code_token_length + input_token_length > remaining_length:\n",
    "#             # 如果总长度超过剩余长度，首先截断较长的部分\n",
    "#             if code_token_length > input_token_length:\n",
    "#                 # 优先截断 code\n",
    "#                 truncated_code = tokenizer.decode(tokenized_code['input_ids'][:remaining_length - input_token_length])\n",
    "#                 truncated_input = data_point[\"report\"]\n",
    "#             else:\n",
    "#                 # 优先截断 input\n",
    "#                 truncated_code = data_point[\"code\"]\n",
    "#                 truncated_input = tokenizer.decode(tokenized_input['input_ids'][:remaining_length - code_token_length])\n",
    "#         else:\n",
    "#             # 如果总长度不超标，不做额外截断\n",
    "#             truncated_code = data_point[\"code\"]\n",
    "#             truncated_input = data_point[\"report\"]\n",
    "\n",
    "#         # 构建最终截断后的 prompt\n",
    "#         full_prompt = f\"\"\"You are a developer of the GCC compiler. Your job is to categorize bug reports. You are given a snippet of code that triggers the bug and a description of the bug.\n",
    "# The bug reports are categorized as follows:'code-simplification-optimization-defects','control-flow-optimization-defects','data-flow-analysis-optimization-defects','infrastructure-defects','interprocedural-optimization-defects','memory-optimization-defects','numerical-analysis-optimization-defects','vectorization-defects'.\n",
    "# ### Code Snippet:\n",
    "# {truncated_code}\n",
    "# ### Bug Description:\n",
    "# {truncated_input}\n",
    "# ### Response:\n",
    "# {data_point[\"category\"]}\n",
    "# \"\"\"\n",
    "    \n",
    "#     # 进行最后的tokenize处理，确保token长度满足要求\n",
    "#     return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032344ac-4d51-4e68-88af-74d983f0ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动截断code/input\n",
    "\n",
    "MAX_LENGTH = 2900  # Llama3.1支持128k上下文\n",
    "\n",
    "def tokenize_and_truncate(data_point):\n",
    "    # 构建初始的 full_prompt\n",
    "    full_prompt = f\"\"\"You are a developer of the GCC compiler. Your job is to categorize bug reports. You are given a bug description.\n",
    "The bug reports are categorized as follows:'code-simplification-optimization-defects','control-flow-optimization-defects','data-flow-analysis-optimization-defects','infrastructure-defects','interprocedural-optimization-defects','memory-optimization-defects','numerical-analysis-optimization-defects','vectorization-defects'.\n",
    "### Bug Description:\n",
    "{data_point[\"text\"]}\n",
    "### Response:\n",
    "{data_point[\"category\"]}\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize整个prompt\n",
    "    tokenized_prompt = tokenizer(full_prompt)\n",
    "    token_length = len(tokenized_prompt['input_ids'])\n",
    "\n",
    "    # 如果超过 MAX_LENGTH，截断处理\n",
    "    if token_length > MAX_LENGTH:\n",
    "        tokenized_input = tokenizer(data_point[\"text\"], truncation=False)\n",
    "        truncated_input = tokenizer.decode(tokenized_input['input_ids'][:MAX_LENGTH])\n",
    "\n",
    "        # 构建最终截断后的 prompt\n",
    "        full_prompt = f\"\"\"You are a developer of the GCC compiler. Your job is to categorize bug reports. You are given a bug description.\n",
    "The bug reports are categorized as follows:'code-simplification-optimization-defects','control-flow-optimization-defects','data-flow-analysis-optimization-defects','infrastructure-defects','interprocedural-optimization-defects','memory-optimization-defects','numerical-analysis-optimization-defects','vectorization-defects'.\n",
    "### Bug Description:\n",
    "{truncated_input}\n",
    "### Response:\n",
    "{data_point[\"category\"]}\n",
    "\"\"\"\n",
    "    \n",
    "    # 进行最后的tokenize处理，确保token长度满足要求\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81974d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_truncate)\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_and_truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb11905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    # eval_dataset=tokenized_validation_dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, \n",
    "    dataset_text_field = \"labels\",\n",
    "    dataset_num_proc = 2,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 1000,\n",
    "        num_train_epochs = 25,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps =50,\n",
    "        # eval_strategy=\"steps\",\n",
    "        # eval_steps=50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type =\"linear\",\n",
    "        output_dir = \"output/Llama3.1-ensemble-text-v1-4\",\n",
    "        report_to=\"none\",  # 关闭 WandB 等日志记录\n",
    "    ),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508db21-c488-47f9-8e18-dc4d7a710293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783fa59-7de1-4969-b228-e058d26a2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "save = \"save/Llama3.1-ensemble-text-v1-4\"\n",
    "model.save_pretrained(save)\n",
    "tokenizer.save_pretrained(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674638b-6817-4533-b67a-bfc0d4dabff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
